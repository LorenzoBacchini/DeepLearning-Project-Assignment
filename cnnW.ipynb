{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b876e863",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "cnn architecture to classify numbers from 0 to 999\n",
    "\n",
    "this classifier uses a dataset made of concatenated images on the horizontal axis (width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_datasets\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "importlib.reload(utils_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72a43e",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d72426",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # first convolutional block\n",
    "        # initial size = 1x32x32\n",
    "        self.conv_block1 = nn.Sequential(nn.Conv2d(in_channels= 1, out_channels=32, kernel_size=5, stride=1, padding=2), #32x32x32 [(Wâˆ’K+2P)/S]+1\n",
    "                                         nn.BatchNorm2d(32),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.MaxPool2d(2, 2)) #32x16x16\n",
    "        # second convolutional block\n",
    "        self.conv_block2 = nn.Sequential(nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2), #64x16x16\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.MaxPool2d(2, 2)) #64x8x8\n",
    "        # third convolutional block\n",
    "        self.conv_block3 = nn.Sequential(nn.Conv2d(in_channels=64, out_channels=256, kernel_size=5, stride=1, padding=2), #256x8x8\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.MaxPool2d(2, 2)) #256x4x4\n",
    "        # fully connected blocks\n",
    "        self.fc1 = nn.Flatten(1)\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5))\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5))\n",
    "        self.fc4 = nn.Linear(1024, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = LeNet5().to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1aa69",
   "metadata": {},
   "source": [
    "### TensorBoard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list containing all the classes\n",
    "classes = []\n",
    "for i in range (0,1000):\n",
    "    classes.append(str(i))\n",
    "\n",
    "# log directory in which tensorboard save statistics\n",
    "writer = SummaryWriter('./runs/large_mnist')\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0db840",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b31438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72acb411",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "\n",
    "# transform to apply to the train dataset to retrieve data\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(utils_datasets.mnist_mean, utils_datasets.mnist_std)\n",
    "])\n",
    "\n",
    "# function used to denormalize img before plot it \n",
    "def denormalize(img, std, mean):\n",
    "    img = img * std\n",
    "    return img + mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa0a32",
   "metadata": {},
   "source": [
    "defining the dataset and dataloader from which the data will be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ff13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = utils_datasets.TrainDatasetW(transform=transform_train, dataset_size=150000)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de10ce74",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea2681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN!\n",
    "# number of mini-batches after which statistics are printed\n",
    "mini_batch_size = 50\n",
    "# put net into train mode\n",
    "net.train()\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    # variable to store loss accumulation in mini_batch_size mini-batches\n",
    "    running_loss = 0.0\n",
    "    # number of correct prediction in mini_batch_size mini-batches\n",
    "    correct = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # put data on correct device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # computing loss for a set of mini-batches in order to show live statistics during training \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # print statistics\n",
    "        correct += utils.get_num_correct(outputs, labels)\n",
    "        if i % mini_batch_size == mini_batch_size-1:    # print every mini_batch_size mini-batches\n",
    "            # log the gradients\n",
    "            writer.add_figure('gradients',\n",
    "                            utils.add_gradient_hist(net),\n",
    "                            global_step=epoch * len(train_dataloader) + i + 1)\n",
    "            \n",
    "            # log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss / mini_batch_size,\n",
    "                            epoch * len(train_dataloader) + i + 1)\n",
    "            # log the training accuracy\n",
    "            writer.add_scalar('training accuracy',\n",
    "                            correct / (mini_batch_size*inputs.size(0)),\n",
    "                            epoch * len(train_dataloader) + i + 1)\n",
    "\n",
    "            print('[Epoch, It]: [{},{}] Loss: {} Accuracy: {}'.format(epoch + 1, epoch * len(train_dataloader) + i + 1, running_loss / mini_batch_size, correct / (mini_batch_size*inputs.size(0))))\n",
    "\n",
    "            # log prediction vs. real labels comparison on random mini-batches\n",
    "            writer.add_figure('predictions vs. actuals',\n",
    "                            utils.plot_classes_preds(net, inputs, labels, classes, denormalize),\n",
    "                            global_step=epoch * len(train_dataloader) + i + 1)\n",
    "            \n",
    "            # resetting live statistics to zero before the next set of mini-batches \n",
    "            running_loss = 0.0\n",
    "            correct = 0.0  \n",
    "\n",
    "writer.close()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f712418c",
   "metadata": {},
   "source": [
    "### Save/Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b405b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "PATH = './res/LeNet5W_7.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c40748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "PATH = './res/LeNet5W_7.pth'\n",
    "net.load_state_dict(torch.load(PATH, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904e46c",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b75490",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 1000\n",
    "\n",
    "# transform to apply to the test dataset to retrieve data\n",
    "transform_test = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((32,32)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(utils_datasets.mnist_mean, utils_datasets.mnist_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c553b57",
   "metadata": {},
   "source": [
    "defining the dataset and dataloader from which the data will be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6445ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = utils_datasets.TestDataset('./data/testW/', transform_test)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                              test_batch_size,\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da6641d",
   "metadata": {},
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of correct prediction\n",
    "correct = 0.0 \n",
    "# total number of prediction\n",
    "total = 0.0\n",
    "# number of correct predition every 5 mini-batches\n",
    "running_accuracy = 0.0\n",
    "# number of mini-batches after which statistics are printed\n",
    "mini_batch_size = 5\n",
    "# put net into evaluation mode\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data\n",
    "        # put data on correct device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # picking the class with the highest value as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # updating the stats\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "        # print statistics\n",
    "        if i % mini_batch_size == mini_batch_size-1:    # print every mini_batch_size mini-batches\n",
    "            writer.add_scalar(\n",
    "                'testing accuracy',\n",
    "                running_accuracy / (mini_batch_size*labels.size(0)),\n",
    "                i+1\n",
    "            )\n",
    "\n",
    "            print(f'[batch {i + 1:5d}] accuracy: {100 * running_accuracy // (mini_batch_size * labels.size(0))} %')\n",
    "            running_accuracy = 0.0\n",
    "\n",
    "writer.close()\n",
    "print(f'Accuracy of the network on the {total} test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2749a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of correct prediction\n",
    "correct = 0.0 \n",
    "# total number of prediction\n",
    "total = 0.0\n",
    "# number of correct predition every 5 mini-batches\n",
    "running_accuracy = 0.0\n",
    "# total erroneous prediction\n",
    "err = 0\n",
    "# erroneous prediction on one digit numbers\n",
    "unit_err = 0\n",
    "# erroneous prediction on two digit numbers\n",
    "dec_err = 0\n",
    "# erroneous prediction on three digit numbers\n",
    "cent_err = 0\n",
    "# number of mini-batches after which statistics are printed\n",
    "mini_batch_size = 5\n",
    "# put net into evaluation mode\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data\n",
    "        # put data on correct device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # picking the class with the highest value as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # updating the stats\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_accuracy += (predicted == labels).sum().item()\n",
    "        for p, l in zip(predicted, labels):\n",
    "            if p != l:\n",
    "                err += 1\n",
    "                if l < 10:\n",
    "                    unit_err +=1\n",
    "                elif l < 100:\n",
    "                    dec_err +=1\n",
    "                else: \n",
    "                    cent_err +=1\n",
    "\n",
    "        # print the statistics\n",
    "        if i % mini_batch_size == mini_batch_size-1:    # print every mini_batch_size mini-batches\n",
    "            print(f'[batch {i + 1:5d}] accuracy: {100 * running_accuracy // (mini_batch_size * labels.size(0))} %')\n",
    "            print(f'[batch {i + 1:5d}] total error: {err}, unit_err: {unit_err}, dec_err: {dec_err}, cent_err: {cent_err}')\n",
    "            running_accuracy = 0.0\n",
    "            \n",
    "print(f'Accuracy of the network on the {total} test images: {100 * correct // total} %')\n",
    "print(f'err: {err}, unit_err: {unit_err}, dec_err: {dec_err}, cent_err: {cent_err}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c245a87",
   "metadata": {},
   "source": [
    "### Computing precision-recall curve for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1a485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_probs = []\n",
    "class_label = []\n",
    "with torch.no_grad():\n",
    "    # for each sample in the test dataset\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = net(images)\n",
    "        # calculating the probability of a sample to belong the predicted class\n",
    "        class_probs_batch = [F.softmax(el.cpu(), dim=0) for el in output]\n",
    "\n",
    "        # creating two parallel list, containing the sample probability of belonging to the predicted class \n",
    "        # and the actual sample class respectively\n",
    "        class_probs.append(class_probs_batch)\n",
    "        class_label.append(labels.cpu())\n",
    "\n",
    "# converting the lists into tensors\n",
    "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "test_label = torch.cat(class_label)\n",
    "\n",
    "# plot the pr curves for all the classes\n",
    "for i in range(len(classes)):\n",
    "    utils.add_pr_curve_tensorboard(i, test_probs, test_label, classes, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.predict_img('./data/TestW/797/7.png', net, transform_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
