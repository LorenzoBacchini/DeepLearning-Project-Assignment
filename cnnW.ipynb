{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b876e863",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "cnn architecture to classify numbers from 0 to 999\n",
    "\n",
    "this classifier uses a dataset made of concatenated images on the horizontal axis (width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72a43e",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d72426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (conv_block1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_block2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_block3): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (fc3): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (fc4): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # first convolutional block\n",
    "        # initial size = 1x32x32\n",
    "        self.conv_block1 = nn.Sequential(nn.Conv2d(in_channels= 1, out_channels=32, kernel_size=5, stride=1, padding=2), #32x32x32 [(Wâˆ’K+2P)/S]+1\n",
    "                                         nn.BatchNorm2d(32),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.MaxPool2d(2, 2)) #32x16x16\n",
    "        # second convolutional block\n",
    "        self.conv_block2 = nn.Sequential(nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2), #64x16x16\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.MaxPool2d(2, 2)) #64x8x8\n",
    "        # third convolutional block\n",
    "        self.conv_block3 = nn.Sequential(nn.Conv2d(in_channels=64, out_channels=256, kernel_size=5, stride=1, padding=2), #256x8x8\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.MaxPool2d(2, 2)) #256x4x4\n",
    "        # fully connected blocks\n",
    "        self.fc1 = nn.Flatten(1)\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5))\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3))\n",
    "        self.fc4 = nn.Linear(1024, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = LeNet5().to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0db840",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b31438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72acb411",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "test_batch_size = 1000\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(utils_datasets.mnist_mean, utils_datasets.mnist_std)\n",
    "])\n",
    "\n",
    "def denormalize(img, std, mean):\n",
    "    img = img * std\n",
    "    return img + mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa0a32",
   "metadata": {},
   "source": [
    "defining the dataset and dataloader from which the data will be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ff13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = utils_datasets.DynamicWMNIST(transform=transform_train, dataset_size=150000)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de10ce74",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea2681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN!\n",
    "# put net into train mode\n",
    "net.train()\n",
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # put data on correct device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 50:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f712418c",
   "metadata": {},
   "source": [
    "### Save/Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b405b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "PATH = './res/LeNet5W_3.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "12c40748",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LeNet5:\n\tsize mismatch for conv_block3.0.weight: copying a param with shape torch.Size([256, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([128, 64, 5, 5]).\n\tsize mismatch for conv_block3.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for conv_block3.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for conv_block3.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for conv_block3.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for conv_block3.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for fc2.0.weight: copying a param with shape torch.Size([2048, 4096]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[213]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[32m      2\u001b[39m PATH = \u001b[33m'\u001b[39m\u001b[33m./res/LeNet5W_4.pth\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:2624\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2616\u001b[39m         error_msgs.insert(\n\u001b[32m   2617\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2618\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2619\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2620\u001b[39m             ),\n\u001b[32m   2621\u001b[39m         )\n\u001b[32m   2623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2624\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2625\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2626\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2627\u001b[39m         )\n\u001b[32m   2628\u001b[39m     )\n\u001b[32m   2629\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for LeNet5:\n\tsize mismatch for conv_block3.0.weight: copying a param with shape torch.Size([256, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([128, 64, 5, 5]).\n\tsize mismatch for conv_block3.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for conv_block3.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for conv_block3.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for conv_block3.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for conv_block3.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for fc2.0.weight: copying a param with shape torch.Size([2048, 4096]) from checkpoint, the shape in current model is torch.Size([2048, 2048])."
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "PATH = './res/LeNet5W_4.pth'\n",
    "net.load_state_dict(torch.load(PATH, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904e46c",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b75490",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((32,32)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(utils_datasets.mnist_mean, utils_datasets.mnist_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c553b57",
   "metadata": {},
   "source": [
    "defining the dataset and dataloader from which the data will be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6445ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = utils_datasets.testMNISTDataset('./data/testWMNIST/', transform_test)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                              test_batch_size,\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da6641d",
   "metadata": {},
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of correct prediction\n",
    "correct = 0.0 \n",
    "# total number of prediction\n",
    "total = 0-0\n",
    "# number of correct predition every 10 mini-batches\n",
    "running_accuracy = 0.0\n",
    "\n",
    "# put net into evaluation mode\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data\n",
    "        # put data on correct device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # picking the class with the highest value as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # updating the stats\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print(f'[batch {i + 1:5d}] accuracy: {100 * running_accuracy // (10 * labels.size(0))} %')\n",
    "            running_accuracy = 0.0\n",
    "\n",
    "print(f'Accuracy of the network on the {total} test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2749a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of correct prediction\n",
    "correct = 0.0 \n",
    "# total number of prediction\n",
    "total = 0.0\n",
    "# number of correct predition every 10 mini-batches\n",
    "running_accuracy = 0.0\n",
    "# total erroneous prediction\n",
    "err = 0\n",
    "# erroneous prediction on one digit numbers\n",
    "unit_err = 0\n",
    "# erroneous prediction on two digit numbers\n",
    "dec_err = 0\n",
    "# erroneous prediction on three digit numbers\n",
    "cent_err = 0\n",
    "# put net into evaluation mode\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data\n",
    "        # put data on correct device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # picking the class with the highest value as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # updating the stats\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_accuracy += (predicted == labels).sum().item()\n",
    "        for p, l in zip(predicted, labels):\n",
    "            if p != l:\n",
    "                err += 1\n",
    "                if l < 10:\n",
    "                    unit_err +=1\n",
    "                elif l < 100:\n",
    "                    dec_err +=1\n",
    "                else: \n",
    "                    cent_err +=1\n",
    "                    \n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print(f'[batch {i + 1:5d}] accuracy: {100 * running_accuracy // (10 * labels.size(0))} %')\n",
    "            print(f'[batch {i + 1:5d}] total error: {err}, unit_err: {unit_err}, dec_err: {dec_err}, cent_err: {cent_err}')\n",
    "            running_accuracy = 0.0\n",
    "            \n",
    "print(f'Accuracy of the network on the {total} test images: {100 * correct // total} %')\n",
    "print(f'err: {err}, unit_err: {unit_err}, dec_err: {dec_err}, cent_err: {cent_err}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
