{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b876e863",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "cnn architecture to classify numbers from 0 to 999\n",
    "\n",
    "this classifier uses a dataset made of concatenated images on the horizontal axis (width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f53b3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a1bea2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'c:\\\\Local Disk D_827202219\\\\bacco\\\\Università\\\\Parma-Bologna\\\\Year 1\\\\s.2\\\\Deep learning\\\\DeepLearning-Project-Assignment\\\\utils.py'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils_datasets\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3477112f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72a43e",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d72426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (conv_block1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_block2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_block3): Sequential(\n",
      "    (0): Conv2d(64, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (fc3): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (fc4): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # first convolutional block\n",
    "        # initial size = 1x32x32\n",
    "        self.conv_block1 = nn.Sequential(nn.Conv2d(in_channels= 1, out_channels=32, kernel_size=5, stride=1, padding=2), #32x32x32 [(W−K+2P)/S]+1\n",
    "                                         nn.BatchNorm2d(32),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.MaxPool2d(2, 2)) #32x16x16\n",
    "        # second convolutional block\n",
    "        self.conv_block2 = nn.Sequential(nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2), #64x16x16\n",
    "                                         nn.BatchNorm2d(64),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.MaxPool2d(2, 2)) #64x8x8\n",
    "        # third convolutional block\n",
    "        self.conv_block3 = nn.Sequential(nn.Conv2d(in_channels=64, out_channels=256, kernel_size=5, stride=1, padding=2), #256x8x8\n",
    "                                         nn.BatchNorm2d(256),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.MaxPool2d(2, 2)) #256x4x4\n",
    "        # fully connected blocks\n",
    "        self.fc1 = nn.Flatten(1)\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5))\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5))\n",
    "        self.fc4 = nn.Linear(1024, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = LeNet5().to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0db840",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b31438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72acb411",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "test_batch_size = 1000\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(utils_datasets.mnist_mean, utils_datasets.mnist_std)\n",
    "])\n",
    "\n",
    "def denormalize(img, std, mean):\n",
    "    img = img * std\n",
    "    return img + mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa0a32",
   "metadata": {},
   "source": [
    "defining the dataset and dataloader from which the data will be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e37ff13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = utils_datasets.DynamicWMNIST(transform=transform_train, dataset_size=150000)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de10ce74",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea2681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN!\n",
    "# put net into train mode\n",
    "net.train()\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # put data on correct device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 50:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f712418c",
   "metadata": {},
   "source": [
    "### Save/Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b405b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "PATH = './res/LeNet5W_7.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12c40748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "PATH = './res/LeNet5W_7.pth'\n",
    "net.load_state_dict(torch.load(PATH, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904e46c",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1b75490",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((32,32)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(utils_datasets.mnist_mean, utils_datasets.mnist_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c553b57",
   "metadata": {},
   "source": [
    "defining the dataset and dataloader from which the data will be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6445ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = utils_datasets.testMNISTDataset('./data/testWMNIST/', transform_test)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                              test_batch_size,\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da6641d",
   "metadata": {},
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of correct prediction\n",
    "correct = 0.0 \n",
    "# total number of prediction\n",
    "total = 0-0\n",
    "# number of correct predition every 10 mini-batches\n",
    "running_accuracy = 0.0\n",
    "\n",
    "# put net into evaluation mode\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data\n",
    "        # put data on correct device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # picking the class with the highest value as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # updating the stats\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print(f'[batch {i + 1:5d}] accuracy: {100 * running_accuracy // (10 * labels.size(0))} %')\n",
    "            running_accuracy = 0.0\n",
    "\n",
    "print(f'Accuracy of the network on the {total} test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2749a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of correct prediction\n",
    "correct = 0.0 \n",
    "# total number of prediction\n",
    "total = 0.0\n",
    "# number of correct predition every 10 mini-batches\n",
    "running_accuracy = 0.0\n",
    "# total erroneous prediction\n",
    "err = 0\n",
    "# erroneous prediction on one digit numbers\n",
    "unit_err = 0\n",
    "# erroneous prediction on two digit numbers\n",
    "dec_err = 0\n",
    "# erroneous prediction on three digit numbers\n",
    "cent_err = 0\n",
    "# put net into evaluation mode\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data\n",
    "        # put data on correct device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # picking the class with the highest value as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # updating the stats\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_accuracy += (predicted == labels).sum().item()\n",
    "        for p, l in zip(predicted, labels):\n",
    "            if p != l:\n",
    "                err += 1\n",
    "                if l < 10:\n",
    "                    unit_err +=1\n",
    "                elif l < 100:\n",
    "                    dec_err +=1\n",
    "                else: \n",
    "                    cent_err +=1\n",
    "                    \n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print(f'[batch {i + 1:5d}] accuracy: {100 * running_accuracy // (10 * labels.size(0))} %')\n",
    "            print(f'[batch {i + 1:5d}] total error: {err}, unit_err: {unit_err}, dec_err: {dec_err}, cent_err: {cent_err}')\n",
    "            running_accuracy = 0.0\n",
    "            \n",
    "print(f'Accuracy of the network on the {total} test images: {100 * correct // total} %')\n",
    "print(f'err: {err}, unit_err: {unit_err}, dec_err: {dec_err}, cent_err: {cent_err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "896c508a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the predicted class is: 797\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.predict_img('./data/TestWMNIST/797/1.png', net, transform_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
